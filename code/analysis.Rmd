---
title: "Laptop Pricing"
author: "Attila Szuts"
date: "02/01/2021"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: yes
editor_options:
  chunk_output_type: console
abstract: This analysis will try to answer how you can price a laptop based on its
  specifications and how you can try to find good deals among them. It uses 1300 data
  points to build a linear regression model on price using different properties like
  company that manufactured it, cpu model, RAM size, etc.
---



# Introduction

Following my interest of tech gadgets I wanted to investigate a question, that was bothering me for a while. How can we accurately tell and compare prices of different laptops? It is hard by itself to price them, as usually they are priced at a premium than similar speced custom built PCs, not only because of the portability but also because the assembly costs. So in order to do this, I decided to build a linear regression model for pricing laptops based on their specifications (screen size, manufacturer, RAM, etc.)

# Data prep 

## Data collection

Data was collected from [kaggle](https://www.kaggle.com/muhammetvarl/laptop-price/), thus I do not know the original source of it. Since this is similar to administrative data, it is likely that the values are accurate and there is no inherent classical measurement error present. However, there can still be some abnormalities that can distort the model. For example it might be very important from a pricing perspective how long ago since the laptop has been released, since prices tend to decrease after the initial hype. Also, it can happen quite easily that someone mistyped something if data was entered manually. The question still remains, which is that is price dependent exclusively on specs? Likely that no, there are other factors at play as well, such as design, build quality, materials used, marketing, etc. Also, there are some important properties not present, like battery size, number of ports, keyboard type, etc but for our purpose it will be a good enough approximation of price.

To get as much information as possible, I will try to use all variables in the model, and I'll only exclude them if I have to.

## Data cleaning

I had to clean almost every variable, to clean numeric variables (RAM, SSD size, HDD size, etc.). Alongside this, I also had to mine information from different variables such as IPS, touchscreen properties of screens, or CPU model, manufacturer, frequency, etc. I created functions that could extract these informations into new ones. 

I recoded the baseline for my variables 

- Operating system -> windows 10
- Company -> Lenovo
- Type -> Notebook
- Screen size category -> screen_mid
- Screen resolution -> 1920x1080
- CPU model -> core i7
- Memory type -> ssd
- GPU type -> integrated

Finally, I created an 80-20 train-test split.

```{r include=FALSE}
rm(list = ls())
library(tidyverse); library(janitor); library(estimatr); library(MASS); library(moments); library(car); library(texreg)
laptops <- "data/raw/laptop-price/"
raw1 <- read_csv(paste0(laptops, "laptop_price.csv")) %>% clean_names()
```

```{r include=FALSE}
pattern <- "([:digit:]{3,4}x[:digit:]{3,4})" # pattern to match screen resolutions
  
# data cleaning
laptop <- raw1 %>% 
  mutate(ram = as.numeric(gsub("GB", "", raw1$ram)), # convert RAM to numeric, in GB
         weight = as.numeric(gsub("kg", "", raw1$weight))) %>% # convert weight to numeric, in kg
  # create group dummies for screen size
  mutate(screen_small = ifelse(inches < 15, T, F), 
         screen_mid = ifelse(inches >= 15 & inches < 17, T, F),
         screen_big = ifelse(inches >= 17, T, F)) %>% 
  # extract resolution
  mutate(touchscreen = ifelse(grepl("Touchscreen", raw1$screen_resolution), T, F), # touchscreen dummy
         ips = ifelse(grepl("IPS", raw1$screen_resolution), T, F), # IPS display dummy
         resolution = str_extract_all(raw1$screen_resolution, pattern, simplify = T)) %>% 
  dplyr::select(-screen_resolution) %>% 
  mutate(cpu = tolower(trimws(cpu))) %>% # trim whitespace
  add_column(lencpu = sapply(strsplit(raw1$cpu, " "), length)) %>% 
  mutate(op_sys = tolower(ifelse(grepl("macosx", tolower(gsub(" ", "", op_sys))), "macos" , gsub(" ", "", op_sys))))

  
# number of words in cpu column
temp <- laptop %>% 
  group_by(cpu) %>% 
  summarise(count = n())
temp <- temp %>% 
  add_column(lencpu = sapply(strsplit(temp$cpu, " "), length)) %>% 
  arrange(lencpu, cpu)

# function that extract cpu manufacturer, cpu model, cpu model variant, cpu frequency and number of cores
cpuMan <- function(x) {
  outlist <- c()
  for (e in 1:length(x$cpu)) {
    outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[1]])
  }
  return(outlist)
}
cpuModel <- function(x) {
  outlist <- c()
  for (e in 1:length(x$cpu)) {
    if (x$lencpu[e] == 5 & grepl("xeon", x$cpu[e]) != T)  {
      outlist <- c(outlist, paste(strsplit(x$cpu[e], split = " ")[[1]][[2]], strsplit(x$cpu[e], split = " ")[[1]][[3]]))
    } else if (x$lencpu[e] == 4 & x$cpu_manufac[e] == "intel") {
      outlist <- c(outlist, paste(strsplit(x$cpu[e], split = " ")[[1]][[2]], strsplit(x$cpu[e], split = " ")[[1]][[3]]))
    } else {
      outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[2]])
    }
  }
  return(outlist)
}
cpuVariant <- function(x) {
  outlist <- c()
  for (e  in 1:length(x$cpu)) {
    if (x$lencpu[e] == 4 & x$cpu_manufac[e] != "amd") {
      outlist <- c(outlist, NA)
    } else if (x$lencpu[e] == 4) {
      outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[3]])
    } else if (x$lencpu[e] == 5 & grepl("xeon", x$cpu[e]) != T) {
      outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[4]])
    } else if (x$lencpu[e] == 5 & grepl("xeon", x$cpu[e]) == T) {
      outlist <- c(outlist, paste(strsplit(x$cpu[e], split = " ")[[1]][[3]], strsplit(x$cpu[e], split = " ")[[1]][[4]]))
    } else {
      outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[5]])
    }
  }
  return(outlist)
}
cpuFreq <- function(x) {
  outlist <- c()
  for (e in 1:length(x$cpu)) {
    if (x$lencpu[e] == 4) {
      outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[4]])
    } else if (x$lencpu[e] == 5) {
      outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[5]])
    } else {
      outlist <- c(outlist, strsplit(x$cpu[e], split = " ")[[1]][[6]])
    }
  }
  return(outlist)
}
cpuCores <- function(x) {
  outlist <- c()
  for (e in 1:length(x$cpu)) {
    if (x$lencpu[e] == 6) {
      outlist <- c(outlist, paste(strsplit(x$cpu[e], split = " ")[[1]][[3]], strsplit(x$cpu[e], split = " ")[[1]][[4]]))
    } else {
      outlist <- c(outlist, NA)
    }
  }
  return(outlist)
}
# create function that extracts storage size and type
memory <- function(x) {
  outlist <- list()
  ssd <- c()
  hdd <- c()
  for (e in 1:length(x$memory)) {
    if (x$ssd[e] == T & x$hdd[e] == F) {
      ssd <- c(ssd, str_extract(x$memory[e], "[:digit:]+tb|[:digit:]+gb")[[1]])
      hdd <- c(hdd, 0)
    } else if (x$ssd[e] == F & x$hdd[e] == T) {
      hdd <- c(hdd, str_extract(x$memory[e], "[:digit:]+tb|[:digit:]+gb|1\\.0tb")[[1]])
      ssd <- c(ssd, 0)
    } else {
      tryCatch(
        expr = {
          if (grepl("hybrid", x$memory[e])) {
            hdd <- c(hdd, str_extract_all(x$memory[e], "[:digit:]+gb|[:digit:]+\\.*[:digit:]*tb")[[1]][[1]])
            ssd <- c(ssd, 0)
          } else {
            ssd <- c(ssd, str_extract_all(x$memory[e], "[:digit:]+gb|[:digit:]+\\.*[:digit:]*tb")[[1]][[1]])
            hdd <- c(hdd, str_extract_all(x$memory[e], "[:digit:]+gb|[:digit:]+\\.*[:digit:]*tb")[[1]][[2]])
          }},
        error = function(er) {
          message("there was an error")
          print(laptop$memory[e])
          if (x$ssd[e] == T & x$hdd[e] == F) {
            print("ssd")
          } else if (x$ssd[e] == F & x$hdd[e] == T) {
            print("hdd")
          } else {
            print(paste(str_extract_all(x$memory[e], "[:digit:]+gb|[:digit:]+\\.*[:digit:]*tb")[[1]][[1]], str_extract_all(x$memory[e], "[:digit:]+gb|[:digit:]+\\.*[:digit:]*tb")[[1]][[2]]))
          }
          }
        )
    }
  }
  outlist <- list(ssd = ssd, hdd = hdd)
  return(outlist)
}

# create function that extracts gpu manufacturers
gpuMan <- function(x) {
  outlist <- c()
  for (e in 1:length(x$gpu)) {
    outlist <- c(outlist, strsplit(x$gpu[e], split = " ")[[1]][[1]])
  }
  return(outlist)
}

# add new columns with cleaned cpu data
laptop <- laptop %>% 
  add_column(cpu_manufac = cpuMan(laptop)) # extract manufacturer
laptop <- laptop %>% 
  add_column(cpu_model = cpuModel(laptop), # extract cpu model 
             cpu_model_variant = cpuVariant(laptop), # extract cpu model variant
             cpu_freq = cpuFreq(laptop), # extract cpu clock frequency
             cpu_cores = cpuCores(laptop) # extract cpu cores
             ) %>% 
  mutate(cpu_freq = as.numeric(gsub("ghz", "", cpu_freq, ignore.case = T))) %>% 
  dplyr::select(-c(lencpu, cpu_cores, cpu)) %>% 
  # clean memory
  mutate(memory = tolower(trimws(memory)), 
         ssd = ifelse(grepl("ssd|flash|hybrid", memory), T, F), # create ssd dummy
         hdd = ifelse(grepl("hdd|hybrid", memory), T, F), # create hdd dummy
         memory_type = ifelse(ssd == T & hdd == F, "ssd", ifelse(ssd == F & hdd == T, "hdd", ifelse(ssd == T & hdd == T, "both", "none"))))

laptop <- laptop %>% 
  add_column(ssd_size = memory(laptop)["ssd"][[1]], hdd_size = memory(laptop)["hdd"][[1]]) %>%  # add column for ssd and hdd size
  mutate(hdd_size = ifelse(grepl("tb", hdd_size), as.numeric(gsub("tb", "", hdd_size))*1000, as.numeric(gsub("gb", "", hdd_size))), # transform to GB
         ssd_size = ifelse(grepl("gb", ssd_size), as.numeric(gsub("gb", "", ssd_size)), as.numeric(gsub("tb", "", ssd_size))*1000), # transform to GB
         gpu = tolower(trimws(gpu)), 
         gpu_type = ifelse(grepl("intel hd|intel uhd|intel graphics|intel iris", gpu), "integrated", gpu)) %>%   # does the laptop have a graphics card or is it integrated
  dplyr::select(-c(gpu, memory))

# change reference level
laptop <- laptop %>% 
  mutate(company = relevel(as.factor(company), ref = "Lenovo")) %>% 
  pivot_longer(cols = c("screen_small", "screen_mid", "screen_big"), names_to = "screen_category") %>% 
  filter(value == T) %>% 
  mutate(type_name = relevel(as.factor(type_name), ref = "Notebook"),
         op_sys = relevel(as.factor(op_sys), ref = "windows10"),
         screen_category = relevel(as.factor(screen_category), ref = "screen_mid"),
         resolution = relevel(as.factor(resolution), ref = "1920x1080"),
         cpu_model = relevel(as.factor(cpu_model), ref = "core i7"),
         memory_type = relevel(as.factor(memory_type), ref = "ssd"),
         gpu_type = relevel(as.factor(gpu_type), ref = "integrated"),
         ln_price = log(price_euros))
```

```{r include=FALSE}
# create training-test sample
## 80% of the sample size
smp_size <- floor(0.8 * nrow(laptop))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(laptop)), size = smp_size)

laptop_all <- laptop
laptop_test <- laptop[-train_ind, ]
laptop <- laptop[train_ind, ]

# clean directory
rm(temp, pattern, cpuMan, cpuModel, cpuVariant, cpuFreq, cpuCores, raw1)
```

## Descriptives

We can see in the [appendix](#appendix) that the log transformed version of prices is the closest to distribution.

Detailed plots on the pattern of association between log-price and covariates can be found in the [appendix](#appendix).

```{r include=FALSE}
summtable <- t(data.frame(x = c(summary(laptop$price_euros))))
row.names(summtable) <- NULL

skewness(laptop$price_euros)
skewness(sqrt(laptop$price_euros))
skewness(log(laptop$price_euros))
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(summtable, format = "html", col.names = colnames(summtable), caption = "Summary statistics for price")
```

```{r include=FALSE}
scatterfun <- function(x) {
  ggplot(aes(x, ln_price), data = laptop) +
    geom_point(position = "jitter", alpha = 0.4) + 
    geom_smooth(method = "loess") +
    theme_bw()
}
boxfun <- function(x) {
  ggplot(aes(x, ln_price), data = laptop) + 
    geom_boxplot() + 
    theme_bw() + 
    stat_summary(fun.data = give.n, geom = "text")
}
# function to display sample size in boxplots
give.n <- function(x){
   return(c(y = mean(x), label = length(x)))
}
# residuals function
resids <- function(x, y) {
  # x : data to use
  # y : model to use
  # x$reg3_pred_vals <- y$fitted.values
  x$reg3_pred_vals <- predict(y, x)
  x$reg3_res <- x$ln_price - x$reg3_pred_vals
  x$reg3_pred_exp <- exp(x$reg3_pred_vals)
  x$reg3_res_exp <- x$price_euros - x$reg3_pred_exp
  return(x)
}
```

# Model parameters

For the baseline model I am going to use a simple regression of log price on the manufacturer company. I decided to use ln prices, as it was skewed to the left, having a long right tail and this transformation made it normal. This model gives the average price percentage change for each manufacturer compared to a baseline, which in this case is Lenovo.

reg1: ln_price ~ company

```{r include=FALSE}
reg1 <- lm_robust(ln_price ~ company, data = laptop)
summary(reg1)
```

To find out which coefficients provide the best fit for my data I ran a simulation to find the best possible fit using AIC with the `stepAIC` function.

```{r include=FALSE}
reg2 <- lm(ln_price ~ company + type_name + inches + ram + op_sys + weight + 
            screen_category + touchscreen + ips + resolution + cpu_manufac + 
            cpu_model + memory_type + ssd_size + hdd_size + gpu_type, 
          data = laptop)

scope_list = list(upper = ~company + type_name + inches + ram + op_sys + weight + 
                    screen_category + touchscreen + ips + resolution + cpu_manufac + 
                    cpu_model + memory_type + ssd_size + hdd_size + gpu_type, lower = ~1)

step <- stepAIC(reg2, direction="both", scope = scope_list)
step$anova

step_formula <- as.character(formula(step))
step_formula <- paste(step_formula[2], step_formula[1], step_formula[3])
step_formula <- gsub("(\\+ op_sys)|(\\+ resolution)|(\\+ gpu_type)", "", step_formula)
```

However this model contained [multicollinearity](#multicollinearity), so I had to drop some variables. After this step I came up with the final form:

reg3: `r step_formula`

```{r include=FALSE}
reg3 <- lm(formula(step_formula), data = laptop)

laptop <- resids(laptop, reg3)
```

## Model interpretation

This model can be interpreted as giving the price percentage change, when we change a property. $exp(\alpha)$ gives us the average price for a Lenovo Notebook with Windows 10, Intel core i7 CPU, medium sized FullHD display, SSD and integrated GPU. We can see how the price changes, if we change some property of this model or we can also analyse residuals to find the best deal for a certain category.

We can see that among the companies, Lenovo is somewhere in the lower end having quite a few more expensive counterparts (e.g. Apple), but also having more budget options (e.g. Acer). 

Ultrabooks, Gaming, Convertible and workstation products are all significantly more expensive compared to Notebooks. Netbooks are cheaper, but marginally and insignificantly.

Inches, that is screen size is not a significant predictor, however, the categories were significant. Both smaller and larger screens are around 20% more expensive compared to medium sized screens. This is probably because of other factors (like premium devices being smaller so they are more portable or gaming pcs having large screens).

1GB more RAM will cost you 2% more on average.

The Intel core i7 is a very expensive cpu, and virtually any other option will cost you less (sometimes even as much as 70%!). This is a very interesting finding, since the CPU is basically the heart of the machine, if you can find a suitable cpu for your needs, then you can potentially save a lot on a pc. But in this paper I am not going to go into more details on this for obvious reasons.

Having both SSD and HDD will be 23% higher on average compared to having only SSD. However, interestingly 1 GB more storage is associated with just a fraction of a percent higher prices on average.

Our model has a very high $R^2=0.85$ compared to the baseline $R^2=0.16$. I think, this is a relatively accurate and robust result. We will check this with test sample.


standardized beta


```{r results = 'asis', echo = FALSE}
htmlreg(list(reg1, reg3),
        type = "html",
        custom.model.names = c("Baseline - simple linear", "Extended - Multiple regression"),
        caption = "Modelling laptop prices",
        include.ci = FALSE)
```




# Residual analysis

Now that we are sure our model met all our assumptions (that can be checked in the [appendix](#model-assumptions) we can analyse the residuals to find the most valuable deals. I limited the results to be under €2000 because above that it might be that our model is less accurate and there are other factors in pricing premium products as well. Based on this, we get the following results.

```{r include=FALSE}
summary(reg3)

#overpriced
laptop %>% 
  top_n(5, reg3_res) %>% 
  arrange(desc(reg3_res)) %>% 
  dplyr::select(company, product, price_euros, reg3_pred_exp)
# underpriced
underpriced <- laptop %>% 
  filter(price_euros < 2000) %>% 
  top_n(-5, reg3_res) %>% 
  arrange(reg3_res) %>% 
  dplyr::select(company, product, price_euros, reg3_pred_exp)
```

```{r echo=FALSE}
knitr::kable(underpriced, format = "html", col.names = c("Company", "Product Name", "Price - Actual", "Price - Predicted"), caption = "Underpriced laptops" )
```

# Y - Y hat plot

```{r echo=FALSE, message=FALSE, warning=FALSE}
laptop %>% 
  ggplot(aes(reg3_pred_vals, ln_price)) + 
  geom_point() + 
  theme_bw() + 
  labs(x = "Predicted log-price",
       y = "Actual log-price")#,
       #title = "Y - Y hat plot")
```

We can see on this plot, that the predicted values fit the actual values fairly well.

```{r include=FALSE}
laptop %>% 
  ggplot(aes(reg3_pred_exp, price_euros)) + 
  geom_point() + 
  theme_bw() + 
  labs(x = "Predicted price",
       y = "Actual price",
       title = "Y - Y hat plot")
```

Prediction intervals

```{r include=FALSE}
laptop %>% 
  add_column(pred_lwr = predict(reg3, newdata = laptop, interval = "prediction", alpha = 0.05)[,2],
             pred_upr = predict(reg3, newdata = laptop, interval = "prediction", alpha = 0.05)[,3],
             ci_lwr = predict(reg3, newdata = laptop, interval = "confidence", alpha = 0.05)[,2],
             ci_upr = predict(reg3, newdata = laptop, interval = "confidence", alpha = 0.05)[,3])
```

# Appendix

## Log Price distribution

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(aes(price_euros), data = laptop) + geom_histogram(bins = 50, color = "black", fill = "grey") + theme_bw()
ggplot(aes(sqrt(price_euros)), data = laptop) + geom_histogram(bins = 50, color = "black", fill = "grey") + theme_bw()
ggplot(aes(log(price_euros)), data = laptop) + geom_histogram(bins = 50, color = "black", fill = "grey") + theme_bw()
```


## Pattern of association between ln_price and predictors

```{r echo=TRUE, message=FALSE, warning=FALSE}
boxfun(laptop$company)
boxfun(laptop$type_name)
scatterfun(laptop$inches)
boxfun(laptop$screen_category)
boxfun(laptop$touchscreen)
boxfun(laptop$ips)
boxfun(laptop$resolution)
scatterfun(laptop$ram)
scatterfun(laptop$gpu_type)
boxfun(laptop$cpu_manufac)
boxfun(laptop$cpu_model)
scatterfun(laptop$cpu_freq)
boxfun(laptop$memory_type)
scatterfun(laptop$ssd_size)
scatterfun(laptop$hdd_size)
boxfun(laptop$op_sys)
scatterfun(laptop$weight)
```

## Model Assumptions

To see if my model meets all the assumptions of multiple linear regression I will investigate outliers and influential cases, multicollinearity, residuals and the independence of errors.


### Outliers and influential cases

```{r include=FALSE}
laptop$reg3_res <- resid(reg3)
laptop$rstand <- rstandard(reg3)
laptop$rstudent <- rstudent(reg3)
laptop$cookd <- cooks.distance(reg3)
laptop$dfbeta <- dfbeta(reg3)
laptop$dffit <- dffits(reg3)
laptop$leverage <- hatvalues(reg3)
laptop$covratio <- covratio(reg3)

# large residuals
large_resid <- laptop[laptop$rstand > 2 | laptop$rstand < - 2,]
large_resid <- large_resid[!is.na(large_resid$rstand),] %>% 
  dplyr::select(laptop_id, company, product, price_euros, reg3_pred_exp, ln_price, reg3_pred_vals, rstand, cookd, leverage, covratio) %>% 
  arrange(desc(abs(rstand)))

# are large residuals also influential?
k <- length(strsplit(x = gsub("~", "+", step_formula), split = " + ", fixed = T)[[1]]) - 1 # number of predictors variables in model
n <- nrow(laptop) # number of observations


# cooks distance
large_resid[large_resid$cookd > 1,] # empty
cookd_over_thresh <- length(large_resid[large_resid$cookd > 1,]$laptop_id)

# covariance ratio
cvr_upr <- 1 + (3*(k + 1) / n)
cvr_lwr <- 1 - (3*(k + 1) / n)
large_resid %>% 
  mutate(cvr_crit = ifelse(cvr_lwr > covratio | cvr_upr < covratio, T, F)) %>% 
  filter(cvr_crit) %>% 
  dplyr::select(-cvr_crit) %>% 
  arrange(desc(abs(covratio))) %>% view()

# leverage
avg_leverage <- k + 1/n
large_resid[large_resid$leverage > avg_leverage * 2,] # empty
```

There are in total `r nrow(large_resid)` outliers. This in itself is not necessarily a bad thing, it might be that they are very good deals. However it is worth taking a look at them to see if they exert undue influence on the model which could in turn distort the results. Investigating the cook's distance of the outliers we can find that there ar `r cookd_over_thresh` observation above 1 which means they are not influential cases. 

### Multicollinearity

Upon inspecting the first model (reg3), I found that there is multicollinearity among the variables. So I had to investigate which variables cause this, and I found that there was multicollinearity among the operating system, resolution, gpu type and cpu model confounders. I experimented to see if there are some combination of these variables that can be included in the model, without multicollinearity but only the cpu model could be used. So due to this, I had to go back and exclude the aforementioned variables. But even with this modification, there still may be some multicollinearity among my variables, as the average VIF is above 1.

Multicollinearity is an issue, since it limits the $R^2$ of the model and increases the SE of the $\beta$ coefficients. This makes it more difficult to interpret the results and the model parameters will vary a lot based on the sample provided.


```{r include=FALSE}
mean(vif(reg3)) # there are aliased coefficients in the model -> remove them

names(reg3$coefficients[is.na(reg3$coefficients)]) # aliased coefficients

testreg <- lm(ln_price ~ company + type_name + inches + ram + screen_category + memory_type + ssd_size + hdd_size + cpu_model, data = laptop, singular.ok = F)

mean(vif(testreg))

summary(testreg)

rm(testreg)
```

### Residuals - homoskedasticity and normality


```{r echo=FALSE, message=FALSE, warning=FALSE}
laptop %>% 
  ggplot(aes(rstudent)) + 
  geom_histogram(fill = "grey", color = "black") + 
  theme_bw() + 
  labs(x = "Studentized residuals",
       y = "")
```

We can see on this plot, that the residuals are normally distributed, because they follow a bell-shaped curve.

```{r echo=FALSE, message=FALSE, warning=FALSE}
laptop %>% 
  ggplot(aes(reg3_pred_vals, rstudent)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method = "lm", color = "blue") +
  labs(x = "Fitted values",
       y = "Studentized Residuals")
```

And on this plot we can see, that there is no heteroskedasticity or non-linearity in the data: points are scattered all over at random.

### Independent errors

```{r include=FALSE}
car::dwt(reg3) # no autocorrelation almost perfect
```

The Durbin-Watson test is `r car::dwt(reg3)[2]`, so the errors are independent in the sample.
